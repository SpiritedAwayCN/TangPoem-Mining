# 文本检索作业 - 项目报告

石淳安	1800011607

[TOC]

![](pics/demo0.png)

## 项目简介

### Requirements

* python >= 3.6.0
* dearpygui
* scikit-learn
* numpy
* pandas
* tqdm (optional)

其中，dearpygui无法通过```conda```安装，请使用```pip```安装：

```sh
$ pip install dearpygui
```

### 目录结构

```
TextHomework-1800011607-石淳安/
├── data/
│   ├── poem_v2.xlsx				唐诗诗歌数据集文件
│   ├── wordlist_v2.xlsx			词表文件
├── fonts/						   字体文件目录
├── models/
│   ├── poem_tfidf.npz				诗歌的tf-idf矩阵
│   ├── sym_tfidf.npz				同义词tf-idf矩阵
├── src/
│   ├── utils.py					图形界面相关函数、回调等
│   ├── PubRead.py					发布-订阅模型相关类
│   ├── poem/
│   │   ├── constant.py				读取计算TF-IDF, 余弦相似度等矩阵供使用
│   │   ├── Poem.py					诗歌类
│   │   ├── query.py				查询函数
├── config.py						启动相关配置设置
├── main.py						    主程序
|
|── TF-IDF_Digging.ipynb			TF-IDF提取与Hit相关代码，已集成入src/poem/constant.py
|── Fine_Tunning.ipynb				超参数确定与调试
```

### 开源地址

，DDL后可访问。

## 数据挖掘部分——基础

### TF-IDF计算的变化

与第9次作业类似，依然进行TF-IDF运算，但有如下几点变化：

* TF-IDF计算剔除了作者行的分词结果，因为对人名分词的结果没有意义，TF-IDF作为直方图特征引入此类噪声性能会明显下降。

```python
split_words = table[table['line_number']!=-1]['words'].str.split(' ', expand=True).stack().rename('word').reset_index() # 去除作者行
split_words = split_words[split_words['word'] != '']
new_data = pd.merge(table['Poem_id'], split_words, left_index=True, right_on='level_0')
```

* 如果一个词出现在某首诗对应的标题中，则对应的TF值乘以3，因为标题信息比内容信息更重要，4句话通常都围绕着标题的主题。同时这将使得最终的检索结果，在标题中出现关键词的诗会更加靠前
* 词语```一作```的tf-idf值仅参与索引时的加权计算，HIT算法与近义词挖掘均已人为排除该词，因为```一作```指代某几个字有另一种写法，在诗歌中大量出现，极度影响HIT算法的主特征向量，也影响近义词挖掘的效率。同时，因为该词意义的特殊性，其无需做近义词挖掘，故而采用此策略。
* 由于矩阵较大（7706x19846）无法完全装入内存，因此采用了```scipy.sparse.csr_matrix```存储，稀疏矩阵的形式极大地降低了内存消耗与计算耗时。
* 为了适应```csr_matrix```矩阵在行切片的高效性，同时迎合```sklearn```库的```cosine_similarity```函数以计算余项相似度，因此矩阵的每一列代表一个词而每一行对应一个文档，因此每一行即是每个文档的特征向量，与课上讲述的行列顺序有所不同。

基于诗词的TF-IDF矩阵被用于：

* 检索时用于加权，对于相同的关键词，其在某文档上的tf-idf值与其在其中的出现次数成正比；
* HIT算法确定关联矩阵。

其中，前者保留了```一作```关键词，使得```一作```可以被检索；而后者去除了```一作```的所有tfidf值，使得HIT算法中```一作```不再能决定诗歌间的关联：

```python
poem_tfidf_ev = poem_tfidf.copy().tolil()
poem_tfidf_ev[:, word_dict['一作']] = 0
poem_tfidf_ev = poem_tfidf_ev.tocsr()
```

### 近义词挖掘

依然可以采用TF-IDF方法，但由于需要比较词与词之间的相似性，因此需要对每个词均定义一个“文档”，如每个需要近义词挖掘的词所有上下文关键词的和作为这个词的“文档”，在此"文档"集上计算各个词在各“文档”中的TF-IDF值，作为各文档的特征向量，然后比较余弦相似度，通过合理地选取阈值以确定近义词。

首先需要给定“上下文”的定义，此处有两个备选项：一是将关键词所在的句子定义为上下文，统计所有句子并集的关键词分布；其二是将诗词作为上下文。但应当注意，由于embedding使用TD-IDF，仅仅只保留了直方图特征而没有考虑其他词语在当前词上下文中的出现顺序，如果选取诗歌，由于上下文过长，使得很多词会更多的趋同化，通过观察以诗歌作为上下文的性能不如以单句作为上下文。

同样，近义词TF-IDF矩阵也有一些细节上的处理：

* 剔除了作者行不参与统计；
* 标题中的关键词TF值不再乘3，对于近义词挖掘标题内容与正文内容重要程度一致；
* 仅对需要做近义词挖掘的词定义“文档”计算TF-IDF，而特征向量包括所有词；
* 近义词挖掘对所有词频数超过10的词进行，且移除了```一作```这个词。

虽然只需要对词频超过10的词语做近义词挖掘，即只占约1/6，但为了方便后续计算，这里依然保存的是包含所有存在的关键词的19846x19846矩阵，由于采用稀疏矩阵保存，所以大量的0对内存与计算耗费几乎没有影响。

```
> <19846x19846 sparse matrix of type '<class 'numpy.float64'>'
	with 436486 stored elements in Compressed Sparse Row format>
```

得到TF-IDF矩阵后，可以计算余弦相似度，较高的相似度即更有可能成为同义词：

```python
sim = cosine_similarity(sym_tfidf, dense_output=False)
sim
```

```
> <19846x19846 sparse matrix of type '<class 'numpy.float64'>'
	with 6724949 stored elements in Compressed Sparse Row format>
```

关于近义词的阈值确定，请参阅后文**近义词挖掘阈值确定**部分.

## 数据挖掘部分——进阶

### 近义词挖掘阈值确定

本部分的代码实现位于```Fine_tunning.ipynb```中。首先可视化所有相似矩阵中所有非零值的分布，集中在很低的区间：

```python
import matplotlib.pyplot as plt
plt.hist(c.sim.data, density=True, bins=100)
plt.show()
```

<img src="pics/demo1.png" style="zoom:67%;" />

获得近义词相似关系矩阵后，需要合理的确定阈值。首先应当明确，最终目的是根据订阅/检索的关键词产生**近义词联想**，同时中文的特点使得多数近义词包含相同的字。由于近义词挖掘是无监督学习，没有人工标注好的数据。因此需要人为确定一个准则：

> **准则1：**给出的近义词对中，包含相同字的词对比例应当尽可能高。

由此，以```0.01```的步长枚举所有阈值，查看比例：

![](pics/demo2.png)

可以发现：尽管左图有几个峰值，但靠后的峰值是基本是因为总数的急剧下降导致的比例异常上升。作为平衡，选取了第一个峰作为阈值，得到0.35：

```python
index = np.argmax(ratio[:38])
thres_list[index]
```

```
> 0.35000000000000003
```

但从直方图可以发现，如果单纯以0.35作为阈值，则预测出的近义词非常少。因此需要有所变通，可以考虑：如果两个词包含相同的字，则阈值可以更加宽裕。

设事件：$A$表示“一个词对是近义词”，事件$B$表示“一个词对包含相同的字”，考虑贝叶斯逆概率公式：
$$
\begin{cases}
P(A|B)=\frac{P(B|A)P(A)}{P(B)}\\
P(A|\bar{B})=\frac{P(\bar{B}|A)P(A)}{P(\bar{B})}
\end{cases}
$$
作商，消去先验概率，得到：
$$
\frac{P(A|B)}{P(A|\bar{B})}=\frac{P(B|A)}{P(\bar{B}|A)}\cdot\frac{P(\bar{B})}{P(B)}
$$
其中：$\frac{P(\bar{B})}{P(B)}\approx 249.384$，可以直接从词表计算得到；而$P(B|A),P(\bar{B}|A)$是两个似然概率，即所有近义词对中，包含相同字的词对比例。这应该有语言统计学的研究，但目前我没有查到这两个先验概率的值。

由于近义词挖掘本质是一个无监督学习，也无法从测试集获取。因此退而求其次，对这两个概率做估算：统计各个阈值下所预测出的近义词对中，包含相同字的词对比例的均值。注意由于阈值较大时总的词对数已经非常少不再具有统计意义，因此只统计了阈值为0.5之前的所有比例：

```python
np.mean(comms[:50] / (totals[:50] - comms[:50]))
```

```
> 0.00790626952644078
```

于是$\frac{P(A|B)}{P(A|\bar{B})}\approx 1.972$，即如果一个词对包含相同的字，则它们是近义词的概率是不包含同字词对的两倍左右。由于先前已经选取了0.35作为阈值，为了对包含同字词对选择更宽裕的阈值，应保证这个更加宽裕的阈值所得到的近义词对数，是先前阈值中非同字词对数的两倍。

> **准则2**：新阈值所得到含同字的近义词对数，约为先前阈值中非同字词对数的2倍。

```python
pair_num = (totals[35] - comms[35])*1.9716986164385752
pair_num, comms[comms >= pair_num]
```

```
> (1135.6984030686194,
 array([13355,  9741,  6801,  4702,  3246,  2254,  1594,  1149]))
```

于是，```1149```对应的阈值```0.07```非常接近。于是0.35作为基本阈值，而若词对包含同字，则以0.07作为阈值。观察一下0.07作为阈值，可以接受：

<img src="pics/demo3.png" style="zoom:67%;" />

> **近义词判定最终策略：**
>
> * 相似度不小于0.35，则为近义词；
> * 或相似度不小于0.07，且词对中两个词包含相同的字，则为近义词。

### 检索排序加权规则确定

在没有近义词时，只需根据关键词对诗歌的tf-idf值求和排序。但由于引入了近义词，因此需要按某种规则进行加权。加权过程可以引入相似度度量。但由于近义词提取中，对同字词对与不同字词对做了区分，因此此处应考虑对近义词进行分级：

* $L_0$级：关键词
* $L_1$级：包含同字的近义词
* $L_2$级：不含同字的近义词

级别越低应越靠前，并适当包含一定的交集。不同的级别有不同的加权因子，加权公式如下：
$$
w_d=\sum_{w\in L_0}\mbox{tf-idf}(d,w) +\sum_{w\in L_1}a_1\sum_{w'\in L_0}\cos(w,w')\cdot \mbox{tf-idf}(d,w)+\sum_{w\in L_2}a_2\sum_{w'\in L_0}\cos(w,w')\cdot \mbox{tf-idf}(d,w)
$$
先统计对于有近义词的关键词，平均匹配上的近义词个数：

```python
np.mean(sim_cnt_avg[sim_cnt_avg > 0])   # --> 1.5
```

平均每个词匹配上1.5个近义词，基于此大致确定如下准则：

> **准则：**
> 在两个词的tf-idf值接近的情况下，如下条件可能使得后一级的词对应诗歌排列在前一级别的关键词前：
>
> 1. **标题中**包含2个以上近义词
> 2. **标题中**包含1个近义词，且正文中多次出现近义词

在TF-IDF特征提取中，题目的TF值被人为地乘了3，因此上述准则约应使得$a_1\sum_{w\in L_1,w'\in L_0}\cos(w,w')\approx 1/6$，计算一下各级相似度的均值：

```python
np.mean(sim_avg), np.mean(sim_avg2)  # --> (0.11309679094428852, 0.44390500631836005)
```

于是：$a_1\approx 1/6\cdot\frac{1}{1.5\times 0.11309679094428852}\approx 0.982\approx 1$

同理，应使得$a_2\sum_{w\in L_1,w'\in L_0}\cos(w,w')\approx 1/36$，得到$a_2\approx0.042\approx0.04$

此外，因为标题中的TF值是标题外的3倍，因此在标题中的匹配会更加靠前。如下图，标题中包含的在最前，同时含近义词的会排在单个前（且在标题的更前），往后会有许多没有匹配上关键词但有近义词匹配的。

![](pics/demo4.png)

### HIT算法相关

以上加权规则在关键词近义词出现种类与频率不同时可以作出排序，但实际查询过程中许多诗歌均恰好出现某个词一次，由于TF-IDF只有直方图特征，这导致这些诗歌上该词的TF-IDF值完全一致，导致诗歌的权重一致。对于此类诗歌，没有指标进行内部的排序，此时可以采用HIT算法。

以下将论证：HIT算法得出的权值**不能完全替代**近义词作为排序依据。

由于关联矩阵基于所有词的TF-IDF，因此词语直方图相近的诗歌会有更高的关联度。但直方图相近不代表所查询的词相近。HIT算法最终使得关联度高的诗歌具有更大的权重，一定程度上缩小了特定的关键词词频信息，最终若按HIT权值排序，靠前的诗歌通常意象相近，且有诗歌集中话题的共性，而并非关键词出现最多。

因此采用的如下策略：

> **Hit优化排序依据：**权重作为第一关键字排序，相近的情况下，HIT算法得出的权值作为第二关键词排序。

```python
def poem_cmp(a, b):
    a, b = a[1], b[1]
    if abs(a[0] - b[0]) <= 1e-6:
        return b[1] - a[1]
    else:
        return b[0] - a[0]
  
self.poems = sorted(res, key=cmp_to_key(poem_cmp))
```

由于Hit作为第二关键字，所以参数选取较为随意，先观察相似矩阵的非零值分布：

<img src="pics/demo5.png" style="zoom:67%;" />

控制约5%的比例，相关，得到阈值0.08作为关联矩阵阈值，随后二值化迭代求主特征向量，后通过各诗歌与该向量的余弦相似度作为排序依据。优化后对比如下，右侧为使用HIT优化，左侧为不使用：

![](pics/demo6.png)

以上是以“明月”为关键词进行订阅，图中展开的诗歌是权值相等（正文中恰出现一次明月）的订阅排序最前的两首，未使用Hit优化的排序较为随机，而使用Hit优化后，第一首《夜泊江渚》与非常符合明月相关的意象（包括“夜泊”、“江”、“水”等，以及第二首《有寄》的“山”、“云”等）。

由上看出，在权重一致的情况下，HIT优化使得更接近主题向量的诗歌排序在最前。

## 项目部分

### 发布-订阅者模型

所有诗歌信息保存于发布者中，有新的订阅者时，发布者根据参数进行检索，后将结果发送给订阅者，订阅者自行保存诗歌内容。于是定义信息传输协议：

订阅者调用订阅发布者函数时，传入所有订阅相关参数，函数按如下格式返回结果：

```python
List(str), List(Poem, (float, float)) | None
```

如果发布者若查询到结果，则返回一个同义词表（用于订阅者高亮关键词）与一个诗歌列表，诗歌列表包含一个有两个```float```元组，分别代表排序的第一关键字与第二关键字（不保证列表有序），以及一个Poem对象，包含是个的所有信息（包括分词信息）。

于是，查询函数有发布者调用，图形界面绑定订阅者，形成类似前后端的分离。同时此类交互具有可扩展性，如果发布者随后发布了姓氏歌，则只需返回```Poem, (float, float)```形式的数据，订阅者可根据元组的数值插入排序。本项目未完全实现后续追加诗歌的功能，但已预留接口。

### DearPyGui库

本项目使用[DearPyGui](https://hoffstadt.github.io/DearPyGui/)作为图形界面开发库，请注意安装最新版本，否则可能无法正确显示中文字符。
https://github.com/hoffstadt/DearPyGui

#### 优势与特色

简单好用是其最大的优势、开发文档直观清晰、所有widgets基于名称字典管理构成树状结构，库根据树结构自动绘制排版。同时内置有许多强大的功能（如plot、调色板等），可以轻易实现复杂的绘图、图形学相关界面。

由于组件的访问、修改完全基于字符串名称，所与定义合理的组件命名规则可极大地降低开发难度。

#### 缺陷

没有Layout，高级接口只支持左对齐，但有许多简易的组件用于微调排版，但无法实现居中等自动layout，参见[Issue #670](https://github.com/hoffstadt/DearPyGui/issues/670)。仅基于组件名称（字符串）的管理过于灵活，调试困难可靠性较低。

#### 一个小细节

如前所述，高亮词汇在reader进行，目的是待高亮关键词需要在图形界面加入单独的```add_text```组件，由于高亮关键词涉及与GUI库的交互，是一个较为耗时的过程。对于返回结果很多的时候直接全部高亮是一个耗时的过程（约3秒），甚至远超检索用时，而事实上对于上千条的返回结果用户并不一定全部查看，因此采用Lazy策略：

> **Lazy策略**：所有页面不立即进行定义与绘制，直到其不得不绘制。

由于此GUI库基于字符串名称管理组件，因此只需要按照组件字符串定义规则，直接通过字符串查询（如查询几号订阅的第几页是否已经绘制）。对于没有绘制的窗口，进行绘制；对于已绘制的窗口，将可见性设为```True```。如此，极大地降低了新增订阅工作的执行时间。

```python
if does_item_exist(f'{ws[0]}_p{new_page}'):
    show_item(f'{ws[0]}_p{new_page}')
else:
    build_page(get_item_callback_data(ws[0] + '_selectpage'), new_page, parent=ws[0] + '_pannel')
```

其余特性不再赘述，可运行程序尝试挖掘。

## 分析与总结

本项目使用TD-IDF做Embedding分析近义词与诗歌特征，以dearpygui作为图形界面库，实现了一个简易诗词订阅与检索程序。查询基于一定的准则确定参数，并引入了可选的Hit优化排序，最终得到了Acceptable的查询结果与排列顺序。

但对于检索还有优化的空间，在多关键词时，应当还有一个统计量代表匹配上的关键词个数，目前的加权方法只是单纯的加性而未考虑这一点，实现这一点应当需要在排序指标中引入非线性项，是一个值得深入讨论的空间。

